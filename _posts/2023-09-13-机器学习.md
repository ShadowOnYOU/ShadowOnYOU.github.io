---
layout: post
title: 机器学习——KNN算法
categories: [KNN, 机器学习]
description: some word here
keywords: KNN
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---

# 机器学习——KNN算法

## 1. k-近邻分类器

### 1.1 算法流程

1. 计算测试样本中所有训练样本xi之间的距离
2. 对所有距离值（相似度值）进行升序（降序）排列
3. 选择k个最近（距离最小/相似度最大）的训练样本
4. 采用投票法，将近邻中样本数最多的类别标签分配给x拔

### 1.2 k的取值的影响

1. k一般取奇数值
2. k取不同的值可能会导致不同的分类结果
3. k较小时，对噪声敏感，整体模型变得复杂，容易过拟合
4. k较大时，对噪声不敏感，整体模型变得简单，容易牵拟合

## 2. 最近邻分类器

### 2.1 算法流程

1. 计算测试样本中所有训练样本之间的距离d
2. 对所有距离值进行升序排列
3. 选择最近的那个训练样本![截屏2023-09-13 16.59.57](https://github.com/ShadowOnYOU/images/blob/main/test202309131700971.png?raw=true)
4. 将xi*的类别标签分配给x拔

### 2.2 泛化错误率

![image-20230913170200022](https://github.com/ShadowOnYOU/images/blob/main/test202309131702049.png?raw=true)

![image-20230913170526607](https://github.com/ShadowOnYOU/images/blob/main/test202309131705633.png?raw=true)

**结论：**最近邻分类器虽然简单，但是它的泛化错误率不超过贝叶斯分类器的错误率两倍。

## 3. k-近邻回归

### 3.1 算法流程

1. 计算测试样本中所有训练样本之间的距离d
2. 对所有距离值进行升序排列
3. 选择k个最近的训练样本
4. 将距离值的倒数作为权重，然后将k个近邻的标签值**加权平均**，作为x拔的预测值。

### 3.2 优点&缺点

优点：精度高，对异常值不敏感，无数据输入假定

缺点：计算复杂度高，空间复杂度高

## 4. 降低近邻计算

### 4.1 维诺图

### 4.2 KD树

### 4.3 降维

### 4.4 哈希

